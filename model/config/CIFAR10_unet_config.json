{"layer_channels": [3, 8, 16, 32, 64], "layer_attention": [false, true, false, false], "layer_groups": [1, 1, 1, 1], "time_emb_dim": 128, "time_n": 100000.0, "kernel_size": 2, "upsample_size": 2, "residual": true, "padding": "same", "mlp_layers": null, "num_heads": 8, "embed_dim": 256}
