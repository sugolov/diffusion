{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71aefded-e96c-4f46-bb7e-222cf7623d9a",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7556d0e1-cb1a-44f9-b3c7-0dc9f525a28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import diffusers\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from urllib.request import urlopen \n",
    "import json \n",
    "\n",
    "os.environ['HF_HOME'] = \"/run/media/anton/hdd/hf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7b26fcc-fbd9-4fd9-827a-e355ab9fd2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anton/source/torch-env/lib/python3.12/site-packages/diffusers/models/transformers/transformer_2d.py:34: FutureWarning: `Transformer2DModelOutput` is deprecated and will be removed in version 1.0.0. Importing `Transformer2DModelOutput` from `diffusers.models.transformer_2d` is deprecated and this will be removed in a future version. Please use `from diffusers.models.modeling_outputs import Transformer2DModelOutput`, instead.\n",
      "  deprecate(\"Transformer2DModelOutput\", \"1.0.0\", deprecation_message)\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "import PIL.Image\n",
    "import numpy as np\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"uoft-cs/cifar10\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0bca250e-9961-43f7-ba8c-89422261f519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "batch_size=32\n",
    "\n",
    "preprocess = transforms.Compose(\n",
    "        [\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5], [0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "def transform(examples):\n",
    "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"img\"]]\n",
    "    return {\"images\": images}\n",
    "\n",
    "dataset.set_transform(transform)\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249fa6a5-654b-47ae-a20e-34a28b430249",
   "metadata": {},
   "source": [
    "## Define VAE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "455c6838-72ad-4548-b853-849c95f827dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.models import AutoencoderKL\n",
    "\n",
    "model = AutoencoderKL(\n",
    "    in_channels=3,  \n",
    "    out_channels=3,  \n",
    "    latent_channels=4,\n",
    "    layers_per_block=2,  \n",
    "    block_out_channels=(128, 256, 256, 256,),  # the number of output channels for each UNet block\n",
    "    down_block_types=(\n",
    "        \"DownEncoderBlock2D\",\n",
    "        \"DownEncoderBlock2D\",\n",
    "        \"DownEncoderBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
    "        \"DownEncoderBlock2D\",\n",
    "    ),\n",
    "    up_block_types=(\n",
    "        \"UpDecoderBlock2D\",  # a regular ResNet upsampling block\n",
    "        \"UpDecoderBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
    "        \"UpDecoderBlock2D\",\n",
    "        \"UpDecoderBlock2D\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad4261b9-ef35-4957-9779-de2e5ce614d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.optimization import get_constant_schedule\n",
    "from accelerate import Accelerator\n",
    "\n",
    "lr=1e-3\n",
    "epochs=150\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "lr_scheduler = get_constant_schedule(optimizer=optimizer)\n",
    "\n",
    "accelerator = Accelerator(mixed_precision=\"fp16\", gradient_accumulation_steps=1)\n",
    "\n",
    "model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1a6869f-34cc-433f-b706-e2485c9f12b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "212da8cd-0532-4d7c-9520-c949f0833173",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batch['images']\n",
    "\n",
    "q = model.encode(x).latent_dist\n",
    "\n",
    "loss_kl = -0.5 * torch.sum(1 + q.logvar - q.mean.pow(2) - q.logvar.exp())\n",
    "\n",
    "x_pred = model.decode(q.sample()).sample\n",
    "\n",
    "loss_recon = torch.nn.functional.mse_loss(x_pred, x)\n",
    "\n",
    "loss = loss_recon + loss_kl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ddf083-05f6-42b1-92e1-64c9588d19c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Step [1/1563], Loss: 1080.9672\n",
      "Epoch [1/150], Step [101/1563], Loss: 0.4157\n",
      "Epoch [1/150], Step [201/1563], Loss: 0.3153\n",
      "Epoch [1/150], Step [301/1563], Loss: 0.2510\n",
      "Epoch [1/150], Step [401/1563], Loss: 0.2995\n",
      "Epoch [1/150], Step [501/1563], Loss: 0.2688\n",
      "Epoch [1/150], Step [601/1563], Loss: 0.2866\n",
      "Epoch [1/150], Step [701/1563], Loss: 1.3666\n",
      "Epoch [1/150], Step [801/1563], Loss: 0.4619\n",
      "Epoch [1/150], Step [901/1563], Loss: 0.2741\n",
      "Epoch [1/150], Step [1001/1563], Loss: 0.2582\n",
      "Epoch [1/150], Step [1101/1563], Loss: 0.4918\n",
      "Epoch [1/150], Step [1201/1563], Loss: 0.3504\n",
      "Epoch [1/150], Step [1301/1563], Loss: 0.2481\n",
      "Epoch [1/150], Step [1401/1563], Loss: 0.2373\n",
      "Epoch [1/150], Step [1501/1563], Loss: 0.2582\n",
      "Epoch [2/150], Step [1/1563], Loss: 0.2623\n",
      "Epoch [2/150], Step [101/1563], Loss: 0.2426\n",
      "Epoch [2/150], Step [201/1563], Loss: 0.2919\n",
      "Epoch [2/150], Step [301/1563], Loss: 0.2276\n",
      "Epoch [2/150], Step [401/1563], Loss: 0.2221\n",
      "Epoch [2/150], Step [501/1563], Loss: 0.2532\n",
      "Epoch [2/150], Step [601/1563], Loss: 0.2586\n",
      "Epoch [2/150], Step [701/1563], Loss: 0.2476\n",
      "Epoch [2/150], Step [801/1563], Loss: 0.2646\n",
      "Epoch [2/150], Step [901/1563], Loss: 0.2689\n",
      "Epoch [2/150], Step [1001/1563], Loss: 0.2193\n",
      "Epoch [2/150], Step [1101/1563], Loss: 0.2613\n",
      "Epoch [2/150], Step [1201/1563], Loss: 0.2648\n",
      "Epoch [2/150], Step [1301/1563], Loss: 0.2245\n",
      "Epoch [2/150], Step [1401/1563], Loss: 0.2303\n",
      "Epoch [2/150], Step [1501/1563], Loss: 0.2506\n",
      "Epoch [3/150], Step [1/1563], Loss: 0.2446\n",
      "Epoch [3/150], Step [101/1563], Loss: 0.2419\n",
      "Epoch [3/150], Step [201/1563], Loss: 0.2733\n",
      "Epoch [3/150], Step [301/1563], Loss: 0.2558\n",
      "Epoch [3/150], Step [401/1563], Loss: 0.2742\n",
      "Epoch [3/150], Step [501/1563], Loss: 0.2665\n",
      "Epoch [3/150], Step [601/1563], Loss: 0.2581\n",
      "Epoch [3/150], Step [701/1563], Loss: 0.2380\n",
      "Epoch [3/150], Step [801/1563], Loss: 0.3085\n",
      "Epoch [3/150], Step [901/1563], Loss: 0.2661\n",
      "Epoch [3/150], Step [1001/1563], Loss: 0.2641\n",
      "Epoch [3/150], Step [1101/1563], Loss: 0.2379\n",
      "Epoch [3/150], Step [1201/1563], Loss: 0.2377\n",
      "Epoch [3/150], Step [1301/1563], Loss: 0.2668\n",
      "Epoch [3/150], Step [1401/1563], Loss: 0.2319\n",
      "Epoch [3/150], Step [1501/1563], Loss: 0.2641\n",
      "Epoch [4/150], Step [1/1563], Loss: 0.2454\n",
      "Epoch [4/150], Step [101/1563], Loss: 0.3297\n",
      "Epoch [4/150], Step [201/1563], Loss: 0.2631\n",
      "Epoch [4/150], Step [301/1563], Loss: 0.2555\n",
      "Epoch [4/150], Step [401/1563], Loss: 0.2438\n",
      "Epoch [4/150], Step [501/1563], Loss: 0.2612\n",
      "Epoch [4/150], Step [601/1563], Loss: 0.2680\n",
      "Epoch [4/150], Step [701/1563], Loss: 0.2927\n",
      "Epoch [4/150], Step [801/1563], Loss: 0.2874\n",
      "Epoch [4/150], Step [901/1563], Loss: 0.1945\n",
      "Epoch [4/150], Step [1001/1563], Loss: 0.2382\n",
      "Epoch [4/150], Step [1101/1563], Loss: 0.2578\n",
      "Epoch [4/150], Step [1201/1563], Loss: 0.2306\n",
      "Epoch [4/150], Step [1301/1563], Loss: 0.2873\n",
      "Epoch [4/150], Step [1401/1563], Loss: 0.2274\n",
      "Epoch [4/150], Step [1501/1563], Loss: 0.2526\n",
      "Epoch [5/150], Step [1/1563], Loss: 0.2641\n",
      "Epoch [5/150], Step [101/1563], Loss: 0.2153\n",
      "Epoch [5/150], Step [201/1563], Loss: 0.2888\n",
      "Epoch [5/150], Step [301/1563], Loss: 0.2853\n",
      "Epoch [5/150], Step [401/1563], Loss: 0.2330\n",
      "Epoch [5/150], Step [501/1563], Loss: 0.2225\n",
      "Epoch [5/150], Step [601/1563], Loss: 0.2135\n",
      "Epoch [5/150], Step [701/1563], Loss: 0.2877\n",
      "Epoch [5/150], Step [801/1563], Loss: 0.2523\n",
      "Epoch [5/150], Step [901/1563], Loss: 0.2808\n",
      "Epoch [5/150], Step [1001/1563], Loss: 0.2471\n",
      "Epoch [5/150], Step [1101/1563], Loss: 0.2509\n",
      "Epoch [5/150], Step [1201/1563], Loss: 0.2254\n",
      "Epoch [5/150], Step [1301/1563], Loss: 0.2439\n",
      "Epoch [5/150], Step [1401/1563], Loss: 0.2484\n",
      "Epoch [5/150], Step [1501/1563], Loss: 0.2755\n",
      "Epoch [6/150], Step [1/1563], Loss: 0.2594\n",
      "Epoch [6/150], Step [101/1563], Loss: 0.2399\n",
      "Epoch [6/150], Step [201/1563], Loss: 0.2461\n",
      "Epoch [6/150], Step [301/1563], Loss: 0.2741\n",
      "Epoch [6/150], Step [401/1563], Loss: 0.2870\n",
      "Epoch [6/150], Step [501/1563], Loss: 0.2099\n",
      "Epoch [6/150], Step [601/1563], Loss: 0.2472\n",
      "Epoch [6/150], Step [701/1563], Loss: 0.2666\n",
      "Epoch [6/150], Step [801/1563], Loss: 0.2312\n",
      "Epoch [6/150], Step [901/1563], Loss: 0.2513\n",
      "Epoch [6/150], Step [1001/1563], Loss: 0.2413\n",
      "Epoch [6/150], Step [1101/1563], Loss: 0.2195\n",
      "Epoch [6/150], Step [1201/1563], Loss: 0.2454\n",
      "Epoch [6/150], Step [1301/1563], Loss: 0.2823\n",
      "Epoch [6/150], Step [1401/1563], Loss: 0.2333\n",
      "Epoch [6/150], Step [1501/1563], Loss: 0.2627\n",
      "Epoch [7/150], Step [1/1563], Loss: 0.2671\n",
      "Epoch [7/150], Step [101/1563], Loss: 0.2469\n",
      "Epoch [7/150], Step [201/1563], Loss: 0.2335\n",
      "Epoch [7/150], Step [301/1563], Loss: 0.2790\n",
      "Epoch [7/150], Step [401/1563], Loss: 0.2312\n",
      "Epoch [7/150], Step [501/1563], Loss: 0.2679\n",
      "Epoch [7/150], Step [601/1563], Loss: 0.2376\n",
      "Epoch [7/150], Step [701/1563], Loss: 0.2297\n",
      "Epoch [7/150], Step [801/1563], Loss: 0.2660\n",
      "Epoch [7/150], Step [901/1563], Loss: 0.2431\n",
      "Epoch [7/150], Step [1001/1563], Loss: 0.1945\n",
      "Epoch [7/150], Step [1101/1563], Loss: 0.2425\n",
      "Epoch [7/150], Step [1201/1563], Loss: 0.2415\n",
      "Epoch [7/150], Step [1301/1563], Loss: 0.2388\n",
      "Epoch [7/150], Step [1401/1563], Loss: 0.2422\n",
      "Epoch [7/150], Step [1501/1563], Loss: 0.2394\n",
      "Epoch [8/150], Step [1/1563], Loss: 0.2203\n",
      "Epoch [8/150], Step [101/1563], Loss: 0.2201\n",
      "Epoch [8/150], Step [201/1563], Loss: 0.2617\n",
      "Epoch [8/150], Step [301/1563], Loss: 0.2192\n",
      "Epoch [8/150], Step [401/1563], Loss: 0.2633\n",
      "Epoch [8/150], Step [501/1563], Loss: 0.2405\n",
      "Epoch [8/150], Step [601/1563], Loss: 0.2341\n",
      "Epoch [8/150], Step [701/1563], Loss: 0.2500\n",
      "Epoch [8/150], Step [801/1563], Loss: 0.2424\n",
      "Epoch [8/150], Step [901/1563], Loss: 0.2710\n",
      "Epoch [8/150], Step [1001/1563], Loss: 0.2272\n",
      "Epoch [8/150], Step [1101/1563], Loss: 0.2729\n",
      "Epoch [8/150], Step [1201/1563], Loss: 0.2379\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, batch in enumerate(train_dataloader):\n",
    "\n",
    "        x = batch['images']\n",
    "        \n",
    "        with accelerator.accumulate(model):\n",
    "            q = model.encode(x).latent_dist\n",
    "\n",
    "            loss_kl = -0.5 * torch.sum(1 + q.logvar - q.mean.pow(2) - q.logvar.exp())\n",
    "            \n",
    "            x_pred = model.decode(q.sample()).sample\n",
    "            \n",
    "            loss_recon = F.mse_loss(x_pred, x)\n",
    "            \n",
    "            loss = loss_recon + loss_kl\n",
    "\n",
    "            \n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            # accelerator.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{epochs}], Step [{batch_idx+1}/{len(train_dataloader)}], Loss: {loss.item():.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
